{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a207646f",
   "metadata": {},
   "source": [
    "# INFORMASI DATA\n",
    "\n",
    "Taswiyah Marsyah Noor (taswiyah2908@gmail.com)\n",
    "\n",
    "+ **Total Data**: 3658 data\n",
    "+ **Sumber Data**: TrustPilot (Electronics Store Flashbay)\n",
    "+ **Kapasitas Penyimpanan**: 701 KB\n",
    "+ **Library**: Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680db7fd",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6341205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4b9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e7815",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7c76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536b06b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844cb202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great products fast turnaround time!</td>\n",
       "      <td>Date of experience: April 06, 2025</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rachel is the absolute best!</td>\n",
       "      <td>From my first telephone contact with Rachel, i...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Will be using Flashbay again!</td>\n",
       "      <td>My rep, Alex, was great to work with. He was r...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Outstanding customer service</td>\n",
       "      <td>Outstanding customer service. Brian Truong has...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fantastic</td>\n",
       "      <td>Fantastic! Always fast and great service. Will...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>Fast delivery.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>Wonderful service!!</td>\n",
       "      <td>Absolutely incredible experience!!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>Daryl is the best</td>\n",
       "      <td>Daryl get the job done on time and on budget</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>Turnaround time was quick</td>\n",
       "      <td>The order process went smoothly. Product was r...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>Flash drives</td>\n",
       "      <td>Fast service and delivery. Products delivered ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3685 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  \\\n",
       "0     Great products fast turnaround time!   \n",
       "1             Rachel is the absolute best!   \n",
       "2            Will be using Flashbay again!   \n",
       "3             Outstanding customer service   \n",
       "4                                Fantastic   \n",
       "...                                    ...   \n",
       "3680                             Excellent   \n",
       "3681                   Wonderful service!!   \n",
       "3682                     Daryl is the best   \n",
       "3683             Turnaround time was quick   \n",
       "3684                          Flash drives   \n",
       "\n",
       "                                                content sentiment  \n",
       "0                    Date of experience: April 06, 2025       NaN  \n",
       "1     From my first telephone contact with Rachel, i...       NaN  \n",
       "2     My rep, Alex, was great to work with. He was r...       NaN  \n",
       "3     Outstanding customer service. Brian Truong has...       NaN  \n",
       "4     Fantastic! Always fast and great service. Will...       NaN  \n",
       "...                                                 ...       ...  \n",
       "3680                                     Fast delivery.   neutral  \n",
       "3681                 Absolutely incredible experience!!   neutral  \n",
       "3682       Daryl get the job done on time and on budget   neutral  \n",
       "3683  The order process went smoothly. Product was r...   neutral  \n",
       "3684  Fast service and delivery. Products delivered ...   neutral  \n",
       "\n",
       "[3685 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2c283",
   "metadata": {},
   "source": [
    "## PELABELAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdffdbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def vader_sentiment(text):\n",
    "    score = sia.polarity_scores(text)\n",
    "    compound = score['compound']  # nilai gabungan dari semua skor\n",
    "    if compound >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "df['sentiment'] = df['content'].astype(str).apply(vader_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2fcda",
   "metadata": {},
   "source": [
    "Melakukan pelabelan data hasil scraping menggunakan VADER, dengan penentuan skor sentimen berdasarkan bobot yang telah ditentukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da711dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    2580\n",
       "neutral     1013\n",
       "negative      92\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782b7ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        168\n",
       "content      171\n",
       "sentiment      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bccf1",
   "metadata": {},
   "source": [
    "Terdapat total 168 data kosong pada kolom title dan 171 data kosong pada kolom content, sehingga saya memutuskan untuk menghapus baris-baris tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9803cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11082691",
   "metadata": {},
   "source": [
    "Melakukan pembersihan data untuk menghilangkan karakter-karakter atau elemen yang tidak relevan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b59be4",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd79fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5e3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].astype(str)\n",
    "# df_tfidf['content'] = df_tfidf['content'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a393b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply ke kolom content\n",
    "df['content'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6205f",
   "metadata": {},
   "source": [
    "# SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bb16bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33813f4",
   "metadata": {},
   "source": [
    "# OVER SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e63b2f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi X_train ke DataFrame karena fit_resample butuh 2D input\n",
    "X_train_df = X_train.to_frame()\n",
    "\n",
    "# Lakukan oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_df, y_train)\n",
    "\n",
    "# Convert kembali ke Series (jika kamu ingin mempertahankan format Series)\n",
    "X_train_resampled = X_train_resampled['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d3101",
   "metadata": {},
   "source": [
    "Karena distribusi label tidak seimbang—dengan jumlah data positif jauh lebih banyak dibandingkan netral dan negatif—saya menerapkan teknik Random Over Sampling untuk menyeimbangkan data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7c87d",
   "metadata": {},
   "source": [
    "# FITUR EKSTRAKSI DAN PEMODELAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76653011",
   "metadata": {},
   "source": [
    "Berikut adalah kombinasi yang saya lakukan pada projek ini\n",
    "+ **1. TF-IDF + Logistic Regression**\n",
    "+ **2. TF-IDF + Dense Layer (MLP dengan Keras)**\n",
    "+ **3. CountVectorizer + Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "588474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_resampled)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45076fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + Logistic Regression — TRAINING\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      1.00      1.00      2064\n",
      "     neutral       1.00      0.99      0.99      2064\n",
      "    positive       0.99      0.99      0.99      2064\n",
      "\n",
      "    accuracy                           0.99      6192\n",
      "   macro avg       0.99      0.99      0.99      6192\n",
      "weighted avg       0.99      0.99      0.99      6192\n",
      "\n",
      "TF-IDF + Logistic Regression — TESTING\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.41      0.39      0.40        18\n",
      "     neutral       0.91      0.85      0.88       169\n",
      "    positive       0.95      0.97      0.96       516\n",
      "\n",
      "    accuracy                           0.92       703\n",
      "   macro avg       0.75      0.74      0.74       703\n",
      "weighted avg       0.92      0.92      0.92       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model_logistic = LogisticRegression(max_iter=1000)\n",
    "model_logistic.fit(X_train_tfidf, y_train_resampled)\n",
    "\n",
    "# === TRAINING EVALUATION ===\n",
    "y_train_pred = model_logistic.predict(X_train_tfidf)\n",
    "print(\"TF-IDF + Logistic Regression — TRAINING\")\n",
    "print(classification_report(y_train_resampled, y_train_pred))\n",
    "\n",
    "# === TESTING EVALUATION ===\n",
    "y_test_pred = model_logistic.predict(X_test_tfidf)\n",
    "print(\"TF-IDF + Logistic Regression — TESTING\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5160d60",
   "metadata": {},
   "source": [
    "**Training**\n",
    "+ Akurasi sangat tinggi: 99%\n",
    "+ Model sangat baik mengenali seluruh kelas pada data latih <br>\n",
    "⚠️ Hasil mendekati sempurna → indikasi overfitting terhadap data training\n",
    "\n",
    "**Testing**\n",
    "+ Akurasi tinggi: 92%\n",
    "+ Mampu menangani kelas dominan (positive) dan cukup baik pada kelas neutral <br>\n",
    "⚠️ Performa pada kelas negative masih rendah → menurunkan keseimbangan prediksi antar kelas <br>\n",
    "⚠️ Ada penurunan akurasi dari training ke testing → overfitting ringan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d293ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "175/175 [==============================] - 3s 9ms/step - loss: 0.3902 - accuracy: 0.8909 - val_loss: 0.0891 - val_accuracy: 0.9839\n",
      "Epoch 2/5\n",
      "175/175 [==============================] - 1s 6ms/step - loss: 0.0240 - accuracy: 0.9957 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "175/175 [==============================] - 1s 5ms/step - loss: 0.0074 - accuracy: 0.9991 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "175/175 [==============================] - 1s 5ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "175/175 [==============================] - 1s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "194/194 [==============================] - 1s 4ms/step\n",
      "TF-IDF + Dense Layer — TRAINING\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00      2064\n",
      "     neutral       1.00      1.00      1.00      2064\n",
      "    positive       1.00      1.00      1.00      2064\n",
      "\n",
      "    accuracy                           1.00      6192\n",
      "   macro avg       1.00      1.00      1.00      6192\n",
      "weighted avg       1.00      1.00      1.00      6192\n",
      "\n",
      "22/22 [==============================] - 0s 7ms/step\n",
      "TF-IDF + Dense Layer — TESTING\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.11      0.18        18\n",
      "     neutral       0.92      0.85      0.88       169\n",
      "    positive       0.94      0.99      0.96       516\n",
      "\n",
      "    accuracy                           0.93       703\n",
      "   macro avg       0.79      0.65      0.67       703\n",
      "weighted avg       0.92      0.93      0.92       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Array\n",
    "X_train_array = X_train_tfidf.toarray()\n",
    "X_test_array = X_test_tfidf.toarray()\n",
    "\n",
    "# Encode label\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train_resampled)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "y_train_cat = to_categorical(y_train_enc)\n",
    "y_test_cat = to_categorical(y_test_enc)\n",
    "\n",
    "# Build model\n",
    "model_dense = Sequential()\n",
    "model_dense.add(Dense(128, activation='relu', input_shape=(X_train_array.shape[1],)))\n",
    "model_dense.add(Dense(64, activation='relu'))\n",
    "model_dense.add(Dense(y_train_cat.shape[1], activation='softmax'))\n",
    "\n",
    "model_dense.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_dense.fit(X_train_array, y_train_cat, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === TRAINING EVALUATION ===\n",
    "y_train_pred = model_dense.predict(X_train_array)\n",
    "y_train_pred_classes = np.argmax(y_train_pred, axis=1)\n",
    "print(\"TF-IDF + Dense Layer — TRAINING\")\n",
    "print(classification_report(y_train_enc, y_train_pred_classes, target_names=le.classes_))\n",
    "\n",
    "# === TESTING EVALUATION ===\n",
    "y_test_pred = model_dense.predict(X_test_array)\n",
    "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
    "print(\"TF-IDF + Dense Layer — TESTING\")\n",
    "print(classification_report(y_test_enc, y_test_pred_classes, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bcb51",
   "metadata": {},
   "source": [
    "**Training**\n",
    "+ Akurasi sempurna: 100%\n",
    "+ Model sangat pas mengenali seluruh kelas pada data latih <br>\n",
    "⚠️ Ini overfitting parah karena model terlalu cocok dengan data training\n",
    "\n",
    "**Testing**\n",
    "+ Akurasi tinggi: 93%\n",
    "+ Performa sangat baik pada kelas dominan (positive) <br>\n",
    "⚠️ Kelas negative sangat buruk dikenali (recall 0.11) <br>\n",
    "⚠️ Performa model tidak seimbang antar kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44db697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_count = cv.fit_transform(X_train_resampled)\n",
    "X_test_count = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a574c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer + Logistic Regression — TRAINING\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      1.00      0.97      2064\n",
      "     neutral       0.99      0.97      0.98      2064\n",
      "    positive       0.99      0.95      0.97      2064\n",
      "\n",
      "    accuracy                           0.97      6192\n",
      "   macro avg       0.97      0.97      0.97      6192\n",
      "weighted avg       0.97      0.97      0.97      6192\n",
      "\n",
      "CountVectorizer + Logistic Regression — TESTING\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.18      0.50      0.26        18\n",
      "     neutral       0.88      0.82      0.85       169\n",
      "    positive       0.96      0.92      0.94       516\n",
      "\n",
      "    accuracy                           0.88       703\n",
      "   macro avg       0.67      0.75      0.68       703\n",
      "weighted avg       0.92      0.88      0.90       703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === TRAINING EVALUATION ===\n",
    "y_train_pred = model_logistic.predict(X_train_count)\n",
    "print(\"CountVectorizer + Logistic Regression — TRAINING\")\n",
    "print(classification_report(y_train_resampled, y_train_pred))\n",
    "\n",
    "# === TESTING EVALUATION ===\n",
    "y_test_pred = model_logistic.predict(X_test_count)\n",
    "print(\"CountVectorizer + Logistic Regression — TESTING\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4089145",
   "metadata": {},
   "source": [
    "**Training**\n",
    "+ Akurasi sangat tinggi: 97%\n",
    "+ Semua kelas dikenali dengan sempurna di data training <br>\n",
    "⚠️ Sangat mungkin overfitting\n",
    "\n",
    "**Testing**\n",
    "+ Akurasi tinggi: 88%\n",
    "+ Performa baik untuk kelas positive dan neutral <br>\n",
    "⚠️ Kelas negative masih lemah (recall hanya 0.50) <br>\n",
    "⚠️ Ada ketidakseimbangan performa antar kelas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce9b23",
   "metadata": {},
   "source": [
    "# SIMPAN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "676f6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan model Dense Layer (MLP) yang sudah dilatih\n",
    "model_dense.save('model_dense_layer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "062dd55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Membuat model Keras untuk Logistic Regression\n",
    "logistic_model_keras = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(X_train_array.shape[1],)),  # Menyesuaikan dengan jumlah fitur pada X_train_tfidf\n",
    "    layers.Dense(3, activation='softmax', use_bias=True, \n",
    "                  kernel_initializer=tf.constant_initializer(model_logistic.coef_.T),  # Bobot dari model Logistic\n",
    "                  bias_initializer=tf.constant_initializer(model_logistic.intercept_))  # Intercept dari model Logistic\n",
    "])\n",
    "\n",
    "# Menyimpan model Logistic Regression dalam format Keras\n",
    "logistic_model_keras.save('model_logistic_regression.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72a0e23",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "018aff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 220 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028F5303DB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Prediksi kelas: positif\n"
     ]
    }
   ],
   "source": [
    "# Daftar label sesuai dengan urutan kelas\n",
    "labels = [\"negatif\", \"netral\", \"positif\"]\n",
    "\n",
    "# Memuat model Dense Layer (MLP) yang sudah disimpan\n",
    "mlp_model_loaded = tf.keras.models.load_model('model_dense_layer.h5')\n",
    "\n",
    "# Misalnya Anda punya data baru (data yang belum pernah dilihat oleh model)\n",
    "# Contoh data baru\n",
    "new_data = [\"wow the goods are very good and the service is okay the goods arrive quickl.\"]\n",
    "\n",
    "# Lakukan transformasi pada data baru dengan TF-IDF Vectorizer (yang sudah dilatih sebelumnya)\n",
    "new_data_tfidf = tfidf.transform(new_data).toarray()\n",
    "\n",
    "# Lakukan prediksi dengan model yang sudah dimuat\n",
    "y_pred_mlp = mlp_model_loaded.predict(new_data_tfidf)\n",
    "\n",
    "# Menentukan kelas dengan probabilitas tertinggi\n",
    "predicted_class_index = np.argmax(y_pred_mlp, axis=1)\n",
    "\n",
    "# Mengonversi indeks kelas ke label\n",
    "predicted_label = labels[predicted_class_index[0]]\n",
    "\n",
    "# Menampilkan hasil prediksi\n",
    "print(f'Prediksi kelas: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4303d127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Prediksi kelas: netral\n"
     ]
    }
   ],
   "source": [
    "# Memuat model yang sudah disimpan\n",
    "logistic_loaded_model = tf.keras.models.load_model('model_logistic_regression.h5')\n",
    "\n",
    "# Data baru untuk prediksi\n",
    "new_data = [\"9 February 2025\"]\n",
    "new_data_tfidf = tfidf.transform(new_data).toarray()\n",
    "\n",
    "# Melakukan prediksi\n",
    "y_pred_log = logistic_loaded_model.predict(new_data_tfidf)\n",
    "\n",
    "# Menentukan kelas dengan probabilitas tertinggi\n",
    "predicted_class_index_log = np.argmax(y_pred_log, axis=1)\n",
    "\n",
    "# Mengonversi indeks kelas ke label\n",
    "predicted_label_log = labels[predicted_class_index_log[0]]\n",
    "\n",
    "# Menampilkan hasil prediksi\n",
    "print(f'Prediksi kelas: {predicted_label_log}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd3cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
